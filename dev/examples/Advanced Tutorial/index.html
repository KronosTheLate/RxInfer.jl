<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced Tutorial · RxInfer.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://biaslab.github.io/RxInfer.jl/examples/Advanced Tutorial/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../assets/header.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="RxInfer.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="RxInfer.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RxInfer.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">User guide</span><ul><li><a class="tocitem" href="../../manuals/getting-started/">Getting started</a></li><li><a class="tocitem" href="../../manuals/model-specification/">Model specification</a></li><li><a class="tocitem" href="../../manuals/constraints-specification/">Constraints specification</a></li><li><a class="tocitem" href="../../manuals/meta-specification/">Meta specification</a></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Inference specification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../manuals/inference/overview/">Overview</a></li><li><a class="tocitem" href="../../manuals/inference/inference/">Static dataset</a></li><li><a class="tocitem" href="../../manuals/inference/rxinference/">Real-time dataset / reactive inference</a></li><li><a class="tocitem" href="../../manuals/inference/manual/">Manual inference specification</a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../library/functional-forms/">Built-in functional form constraints</a></li><li><a class="tocitem" href="../../library/model-specification/">Model specification</a></li><li><a class="tocitem" href="../../library/bethe-free-energy/">Bethe Free Energy</a></li><li><a class="tocitem" href="../../library/exported-methods/">Exported methods</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../Overview/">Overview</a></li><li class="is-active"><a class="tocitem" href>Advanced Tutorial</a><ul class="internal"><li><a class="tocitem" href="#General-model-specification-syntax"><span>General model specification syntax</span></a></li><li><a class="tocitem" href="#Probabilistic-inference-in-RxInfer.jl"><span>Probabilistic inference in RxInfer.jl</span></a></li><li><a class="tocitem" href="#Reactive-Online-Inference"><span>Reactive Online Inference</span></a></li><li><a class="tocitem" href="#Variational-inference"><span>Variational inference</span></a></li></ul></li><li><a class="tocitem" href="../Assessing People Skills/">Assessing People Skills</a></li><li><a class="tocitem" href="../Autoregressive Model/">Autoregressive Model</a></li><li><a class="tocitem" href="../Coin Toss Model/">Coin Toss Model</a></li><li><a class="tocitem" href="../Conjugate-NonConjugate Variational Message Passing/">Conjugate-NonConjugate Variational Message Passing</a></li><li><a class="tocitem" href="../Custom nonlinear node/">Custom nonlinear node</a></li><li><a class="tocitem" href="../GPRegression by SSM/">GPRegression by SSM</a></li><li><a class="tocitem" href="../Gamma Mixture/">Gamma Mixture</a></li><li><a class="tocitem" href="../Gaussian Linear Dynamical System/">Gaussian Linear Dynamical System</a></li><li><a class="tocitem" href="../Gaussian Mixture Univariate/">Gaussian Mixture Univariate</a></li><li><a class="tocitem" href="../Gaussian Mixtures Multivariate/">Gaussian Mixtures Multivariate</a></li><li><a class="tocitem" href="../Global Parameter Optimisation/">Global Parameter Optimisation</a></li><li><a class="tocitem" href="../Handling Missing Data/">Handling Missing Data</a></li><li><a class="tocitem" href="../Hidden Markov Model/">Hidden Markov Model</a></li><li><a class="tocitem" href="../Hierarchical Gaussian Filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../Identification Problem/">Identification Problem</a></li><li><a class="tocitem" href="../Infinite Data Stream/">Infinite Data Stream</a></li><li><a class="tocitem" href="../Invertible Neural Network Tutorial/">Invertible Neural Network Tutorial</a></li><li><a class="tocitem" href="../Kalman filter with LSTM network driven dynamic/">Kalman filter with LSTM network driven dynamic</a></li><li><a class="tocitem" href="../Linear Regression/">Linear Regression</a></li><li><a class="tocitem" href="../Nonlinear Noisy Pendulum/">Nonlinear Noisy Pendulum</a></li><li><a class="tocitem" href="../Nonlinear Rabbit Population/">Nonlinear Rabbit Population</a></li><li><a class="tocitem" href="../Nonlinear Sensor Fusion/">Nonlinear Sensor Fusion</a></li><li><a class="tocitem" href="../Nonlinear Virus Spread/">Nonlinear Virus Spread</a></li><li><a class="tocitem" href="../Probit Model (EP)/">Probit Model (EP)</a></li><li><a class="tocitem" href="../RTS vs BIFM Smoothing/">RTS vs BIFM Smoothing</a></li></ul></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/overview/">Overview</a></li><li><a class="tocitem" href="../../contributing/new-example/">Adding a new example</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Advanced Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/biaslab/RxInfer.jl/blob/main/docs/src/examples/Advanced Tutorial.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>This example has been auto-generated from the <a href="https://github.com/biaslab/RxInfer.jl/tree/main/examples"><code>examples/</code></a> folder at GitHub repository.</p><h1 id="examples-advanced-tutorial"><a class="docs-heading-anchor" href="#examples-advanced-tutorial">Advanced Tutorial</a><a id="examples-advanced-tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#examples-advanced-tutorial" title="Permalink"></a></h1><pre><code class="language-julia hljs"># Activate local environment, see `Project.toml`
import Pkg; Pkg.activate(&quot;.&quot;); Pkg.instantiate();</code></pre><pre><code class="language-julia hljs">using RxInfer, Plots</code></pre><p>This notebook covers the fundamentals and advanced usage of the <code>RxInfer.jl</code> package.</p><p>This tutorial is also available in the <a href="https://biaslab.github.io/RxInfer.jl/stable/">documentation</a>.</p><h2 id="General-model-specification-syntax"><a class="docs-heading-anchor" href="#General-model-specification-syntax">General model specification syntax</a><a id="General-model-specification-syntax-1"></a><a class="docs-heading-anchor-permalink" href="#General-model-specification-syntax" title="Permalink"></a></h2><p>We use the <code>@model</code> macro from the <code>RxInfer.jl</code> package to create a probabilistic model <span>$p(s, y)$</span> and we also specify extra constraints on the variational family of distributions <span>$\mathcal{Q}$</span>, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of <code>RxInfer.jl</code>.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.</p><pre><code class="language-julia hljs"># the `@model` macro accepts a regular Julia function
@model function test_model1(s_mean, s_precision)
    
    # We use the `randomvar` function to create 
    # a random variable in our model
    s = randomvar()
    
    # the `tilde` operator creates a functional dependency
    # between variables in our model and can be read as 
    # `sampled from` or `is modeled by`
    s ~ Normal(mean = s_mean, precision = s_precision)
    
    # We use the `datavar` function to create 
    # observed data variables in our models
    # We also need to specify the type of our data 
    # In this example it is `Float64`
    y = datavar(Float64)
    
    y ~ Normal(mean = s, precision = 1.0)
    
    # It is possible to return something from the model specification (including variables and nodes)
    return &quot;Hello world&quot;
end</code></pre><p>The <code>@model</code> macro creates a function with the same name and with the same set of input arguments as the original function (<code>test_model1(s_mean, s_precision)</code> in this example). The return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.</p><pre><code class="language-julia hljs">modelgenerator = test_model1(0.0, 1.0)

model, returnval = create_model(modelgenerator)</code></pre><pre><code class="nohighlight hljs">(FactorGraphModel(), &quot;Hello world&quot;)</code></pre><p>The benefits of using model generator as a way to create a model is that it allows to change inference constraints and meta specification for nodes. We will talk about factorisation and form constraints and meta specification later on in this demo.</p><p><code>RxInfer.jl</code> returns a factor graph-based representation of a model. We can examine this factor graph structure with the help of some utility functions such as: </p><ul><li><code>getnodes()</code>: returns an array of factor nodes in a corresponding factor graph</li><li><code>getrandom()</code>: returns an array of random variables in the model</li><li><code>getdata()</code>: returns an array of data inputs in the model</li><li><code>getconstant()</code>: returns an array of constant values in the model</li></ul><pre><code class="language-julia hljs">getnodes(model)</code></pre><pre><code class="nohighlight hljs">FactorNodesCollection(nodes: 2)</code></pre><pre><code class="language-julia hljs">getrandom(model) .|&gt; name</code></pre><pre><code class="nohighlight hljs">1-element Vector{Symbol}:
 :s</code></pre><pre><code class="language-julia hljs">getdata(model) .|&gt; name</code></pre><pre><code class="nohighlight hljs">1-element Vector{Symbol}:
 :y</code></pre><pre><code class="language-julia hljs">getconstant(model) .|&gt; getconst</code></pre><pre><code class="nohighlight hljs">3-element Vector{Float64}:
 0.0
 1.0
 1.0</code></pre><p>It is also possible to use control flow statements such as <code>if</code> or <code>for</code> blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the <code>@model</code> block. As an example consider the following (valid!) model:</p><pre><code class="language-julia hljs">@model function test_model2(n)
    
    if n &lt;= 1
        error(&quot;`n` argument must be greater than one.&quot;)
    end
    
    # `randomvar(n)` creates a dense sequence of 
    # random variables
    s = randomvar(n)
    
    # `datavar(Float64, n)` creates a dense sequence of 
    # observed data variables of type `Float64`
    y = datavar(Float64, n)
    
    s[1] ~ Normal(mean = 0.0, precision = 0.1)
    y[1] ~ Normal(mean = s[1], precision = 1.0)
    
    for i in 2:n
        s[i] ~ Normal(mean = s[i - 1], precision = 1.0)
        y[i] ~ Normal(mean = s[i], precision = 1.0)
    end
    
end</code></pre><pre><code class="language-julia hljs">model, _ = create_model(test_model2(10));</code></pre><pre><code class="language-julia hljs"># An amount of factor nodes in generated Factor Graph
getnodes(model) |&gt; length</code></pre><pre><code class="nohighlight hljs">20</code></pre><pre><code class="language-julia hljs"># An amount of random variables
getrandom(model) |&gt; length</code></pre><pre><code class="nohighlight hljs">10</code></pre><pre><code class="language-julia hljs"># An amount of data inputs
getdata(model) |&gt; length</code></pre><pre><code class="nohighlight hljs">10</code></pre><pre><code class="language-julia hljs"># An amount of constant values
getconstant(model) |&gt; length</code></pre><pre><code class="nohighlight hljs">21</code></pre><p>It is also possible to use complex expressions inside the functional dependency expressions</p><pre><code class="language-julia hljs">y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)</code></pre><p>The <code>~</code> operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists</p><pre><code class="language-julia hljs"># s = randomvar() here is optional
# `~` creates random variables automatically
s ~ NormalMeanPrecision(0.0, 1.0)</code></pre><p>An example model which will throw an error:</p><pre><code class="language-julia hljs">@model function error_model1()
    s = 1.0
    s ~ NormalMeanPrecision(0.0, 1.0)
end</code></pre><p>By default the <code>RxInfer.jl</code> package creates new references for constants (literals like <code>0.0</code> or <code>1.0</code>) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. <code>RxInfer.jl</code> will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use <code>constvar()</code> function to create and reuse similar constants in the model specification syntax as</p><pre><code class="language-julia hljs"># Creates constant reference in a model with a prespecified value
c = constvar(0.0)</code></pre><p>An example:</p><pre><code class="language-julia hljs">@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)
    
    s = randomvar(n)
    
    y = datavar(Vector{Float64}, n)
    
    # Here we create constant references
    # for constant matrices in our model 
    # to make inference more memory efficient
    cA = constvar(A)
    cP = constvar(P)
    cQ = constvar(Q)
    
    s[1] ~ MvNormal(mean = zeros(dim), covariance = cP)
    y[1] ~ MvNormal(mean = s[1], covariance = cQ)
    
    for i in 2:n
        s[i] ~ MvNormal(mean = cA * s[i - 1], covariance = cP)
        y[i] ~ MvNormal(mean = s[i], covariance = cQ)
    end
    
end</code></pre><p>The <code>~</code> expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:</p><pre><code class="language-julia hljs">@model function test_model()

    # In this example `ynode` refers to the corresponding 
    # `GaussianMeanVariance` node created in the factor graph
    ynode, y ~ GaussianMeanVariance(0.0, 1.0)
    
    return ynode, y
end</code></pre><h2 id="Probabilistic-inference-in-RxInfer.jl"><a class="docs-heading-anchor" href="#Probabilistic-inference-in-RxInfer.jl">Probabilistic inference in RxInfer.jl</a><a id="Probabilistic-inference-in-RxInfer.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-inference-in-RxInfer.jl" title="Permalink"></a></h2><p><code>RxInfer.jl</code> uses the <code>Rocket.jl</code> package API for inference routines. <code>Rocket.jl</code> is a reactive programming extension for Julia that is higly inspired by <code>RxJS</code> and similar libraries from the <code>Rx</code> ecosystem. It consists of <strong>observables</strong>, <strong>actors</strong>, <strong>subscriptions</strong> and <strong>operators</strong>. For more information and rigorous examples see <a href="https://github.com/biaslab/Rocket.jl">Rocket.jl github page</a>.</p><h3 id="Observables"><a class="docs-heading-anchor" href="#Observables">Observables</a><a id="Observables-1"></a><a class="docs-heading-anchor-permalink" href="#Observables" title="Permalink"></a></h3><p>Observables are lazy push-based collections and they deliver their values over time.</p><pre><code class="language-julia hljs"># Timer that emits a new value every second and has an initial one second delay 
observable = timer(300, 300)</code></pre><pre><code class="nohighlight hljs">TimerObservable(300, 300)</code></pre><p>A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:</p><pre><code class="language-julia hljs">actor = (value) -&gt; println(value)
subscription1 = subscribe!(observable, actor)</code></pre><pre><code class="nohighlight hljs">TimerSubscription()</code></pre><pre><code class="language-julia hljs"># We always need to unsubscribe from some observables
unsubscribe!(subscription1)</code></pre><pre><code class="language-julia hljs"># We can modify our observables
modified = observable |&gt; filter(d -&gt; rem(d, 2) === 1) |&gt; map(Int, d -&gt; d ^ 2)</code></pre><pre><code class="nohighlight hljs">ProxyObservable(Int64, MapProxy(Int64))</code></pre><pre><code class="language-julia hljs">subscription2 = subscribe!(modified, (value) -&gt; println(value))</code></pre><pre><code class="nohighlight hljs">TimerSubscription()</code></pre><pre><code class="language-julia hljs">unsubscribe!(subscription2)</code></pre><pre><code class="language-julia hljs">@model function coin_toss_model(n)

    # `datavar` creates data &#39;inputs&#39; in our model
    # We will pass data later on to these inputs
    # In this example we create a sequence of inputs that accepts Float64
    y = datavar(Float64, n)
    
    # We endow θ parameter of our model with some prior
    θ ~ Beta(2.0, 7.0)
    
    # We assume that the outcome of each coin flip 
    # is modeled by a Bernoulli distribution
    for i in 1:n
        y[i] ~ Bernoulli(θ)
    end
    
    # We return references to our data inputs and θ parameter
    # We will use these references later on during the inference step
    return y, θ
end</code></pre><p>We can call the <code>inference</code> function to run inference in such model:</p><pre><code class="language-julia hljs">p = 0.75 # Bias of a coin

dataset = float.(rand(Bernoulli(p), 500));

result = inference(
    model = coin_toss_model(length(dataset)),
    data  = (y = dataset, )
)

println(&quot;Inferred bias: &quot;, mean_var(result.posteriors[:θ]))</code></pre><pre><code class="nohighlight hljs">Inferred bias: (0.7053045186640472, 0.0004075491266982822)</code></pre><p>We can see that the inferred bias is quite close to the actual value we used in the dataset generation.</p><p>The <code>RxInfer.jl</code> package&#39;s API is more flexible (and reactive!) and can return posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience <code>RxInfer.jl</code> only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model <code>RxInfer.jl</code> exports two functions: </p><ul><li><code>getmarginal(x)</code>: for a single random variable <code>x</code></li><li><code>getmarginals(xs)</code>: for a dense sequence of random variables <code>sx</code></li></ul><p>Let&#39;s see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the <code>Bernoulli</code> distribution with unknown bias parameter <code>θ</code>. To have a fully Bayesian treatment of this problem we endow <code>θ</code> with the <code>Beta</code> prior.</p><pre><code class="language-julia hljs">_, (y, θ) = create_model(coin_toss_model(length(dataset)));</code></pre><pre><code class="language-julia hljs"># As soon as we have a new value for the marginal posterior over the `θ` variable
# we simply print the first two statistics of it
θ_subscription = subscribe!(getmarginal(θ), (marginal) -&gt; println(&quot;New update: mean(θ) = &quot;, mean(marginal), &quot;, std(θ) = &quot;, std(marginal)));</code></pre><p>To pass data to our model we use <code>update!</code> function</p><pre><code class="language-julia hljs">update!(y, dataset)</code></pre><pre><code class="nohighlight hljs">New update: mean(θ) = 0.7053045186640472, std(θ) = 0.020187846014329568</code></pre><pre><code class="language-julia hljs"># It is necessary to always unsubscribe from running observables
unsubscribe!(θ_subscription)</code></pre><pre><code class="language-julia hljs"># The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them
# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing
update!(y, dataset)</code></pre><p><code>Rocket.jl</code> provides some useful built-in actors for obtaining posterior marginals especially with static datasets.</p><pre><code class="language-julia hljs"># the `keep` actor simply keeps all incoming updates in an internal storage, ordered
θvalues = keep(Marginal)</code></pre><pre><code class="nohighlight hljs">KeepActor{Marginal}(Marginal[])</code></pre><pre><code class="language-julia hljs"># `getmarginal` always emits last cached value as its first value
subscribe!(getmarginal(θ) |&gt; take(1), θvalues);</code></pre><pre><code class="language-julia hljs">getvalues(θvalues)</code></pre><pre><code class="nohighlight hljs">1-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=359.0, β=150.0))</code></pre><pre><code class="language-julia hljs">subscribe!(getmarginal(θ) |&gt; take(1), θvalues);</code></pre><pre><code class="language-julia hljs">getvalues(θvalues)</code></pre><pre><code class="nohighlight hljs">2-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=359.0, β=150.0))
 Marginal(Beta{Float64}(α=359.0, β=150.0))</code></pre><pre><code class="language-julia hljs"># the `buffer` actor keeps very last incoming update in an internal storage and can also store 
# an array of updates for a sequence of random variables
θbuffer = buffer(Marginal, 1)</code></pre><pre><code class="nohighlight hljs">BufferActor{Marginal, Vector{Marginal}}(Marginal[#undef])</code></pre><pre><code class="language-julia hljs">subscribe!(getmarginals([ θ ]) |&gt; take(1), θbuffer);</code></pre><pre><code class="language-julia hljs">getvalues(θbuffer)</code></pre><pre><code class="nohighlight hljs">1-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=359.0, β=150.0))</code></pre><pre><code class="language-julia hljs">subscribe!(getmarginals([ θ ]) |&gt; take(1), θbuffer);</code></pre><pre><code class="language-julia hljs">getvalues(θbuffer)</code></pre><pre><code class="nohighlight hljs">1-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=359.0, β=150.0))</code></pre><h2 id="Reactive-Online-Inference"><a class="docs-heading-anchor" href="#Reactive-Online-Inference">Reactive Online Inference</a><a id="Reactive-Online-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Reactive-Online-Inference" title="Permalink"></a></h2><p>RxInfer.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.</p><pre><code class="language-julia hljs">@model function online_coin_toss_model()
    
    # We create datavars for the prior 
    # over `θ` variable
    θ_a = datavar(Float64)
    θ_b = datavar(Float64)
    
    θ ~ Beta(θ_a, θ_b)
    
    y = datavar(Float64)
    y ~ Bernoulli(θ)

end</code></pre><pre><code class="language-julia hljs">autoupdates = @autoupdates begin 
    θ_a, θ_b = params(q(θ))
end</code></pre><pre><code class="nohighlight hljs">(θ_a,θ_b = params(q(θ)),)</code></pre><pre><code class="language-julia hljs">rxresult = rxinference(
    model = online_coin_toss_model(),
    data  = (y = dataset, ),
    autoupdates = autoupdates,
    historyvars = (θ = KeepLast(), ),
    keephistory = length(dataset),
    initmarginals = (
        θ = vague(Beta),
    ),
    autostart = true
);</code></pre><pre><code class="language-julia hljs">animation = @animate for i in 1:length(dataset)
    plot(mean.(rxresult.history[:θ][1:i]), ribbon = std.(rxresult.history[:θ][1:i]), title = &quot;Online coin bias inference&quot;, label = &quot;Inferred bias&quot;, legend = :bottomright)
    hline!([ p ], label = &quot;Real bias&quot;, size = (600, 200))
end

gif(animation, fps = 30)</code></pre><pre><code class="nohighlight hljs">Plots.AnimatedGif(&quot;/tmp/jl_uPqtWH4v2r.gif&quot;)</code></pre><p>In this example we used static dataset and the <code>history</code> field of the reactive inference result, but the <code>rxinference</code> function also supports any real-time reactive stream and can run indefinitely.</p><p>That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, <code>RxInfer</code> is not limited to only the sum-product algorithm but it also supports variational message passing with <a href="https://www.mdpi.com/1099-4300/23/7/807">Constrained Bethe Free Energy Minimisation</a>.</p><h2 id="Variational-inference"><a class="docs-heading-anchor" href="#Variational-inference">Variational inference</a><a id="Variational-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-inference" title="Permalink"></a></h2><p>On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions <span>$q \in \mathcal{Q}$</span>. Often this involves assuming some factorization over <span>$q$</span>. For this purpose the <code>@model</code> macro supports optional <code>where { ... }</code> clauses for every <code>~</code> expression in a model specification.</p><pre><code class="language-julia hljs">@model function test_model6_with_manual_constraints(n)
    τ ~ Gamma(shape = 1.0, rate = 1.0) 
    μ ~ Normal(mean = 0.0, variance = 100.0)
    
    y = datavar(Float64, n)
    
    for i in 1:n
        # Here we assume a mean-field assumption on our 
        # variational family of distributions locally for the current node
        y[i] ~ Normal(mean = μ, precision = τ) where { q = q(y[i])q(μ)q(τ) }
    end

end</code></pre><p>In this example we specified an extra constraints for <span>$q_a$</span> for Bethe factorisation:</p><p class="math-container">\[
q(s) = \prod_{a \in \mathcal{V}} q_a(s_a) \prod_{i \in \mathcal{E}} q_i^{-1}(s_i)
$$


There are several options to specify the mean-field factorisation constraint. 

```julia
y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification
y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification
y[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name
```

It is also possible to use local structured factorisation:

```julia
y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification
y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification
```


### RxInfer.jl constraints macro


`RxInfer.jl` package exports `@constraints` macro to simplify factorisation and form constraints specification. Read more about `@constraints` macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with `@constraints` macro:

```julia
constraints6 = @constraints begin
     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`
end
```

```
Constraints:
  marginals form:
  messages form:
  factorisation:
    q(μ, τ) = q(μ)q(τ)
Options:
  warn = true
```




**Note**: `where` blocks have higher priority over constraints specification

```julia
@model function test_model6(n)
    τ ~ Gamma(shape = 1.0, rate = 1.0) 
    μ ~ Normal(mean = 0.0, variance = 100.0)
    
    y = datavar(Float64, n)
    
    for i in 1:n
        # Here we assume a mean-field assumption on our 
        # variational family of distributions locally for the current node
        y[i] ~ Normal(mean = μ, precision = τ)
    end
    
    return μ, τ, y
end
```



### Inference


To run inference in this model we again need to create a synthetic dataset:

```julia
dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);
```



#### `inference` function


In order to simplify model and inference testing, `RxInfer.jl` exports pre-written inference function, that is aimed for simple use cases with static datasets:


Use `?inference` to quickly check the documentation for the `inference` function.

```julia
result = inference(
    model         = test_model6(length(dataset)),
    data          = (y = dataset, ),
    constraints   = constraints6, 
    initmarginals = (μ = vague(NormalMeanPrecision), τ = vague(GammaShapeRate)),
    returnvars    = (μ = KeepLast(), τ = KeepLast()),
    iterations    = 10,
    free_energy   = true,
    showprogress  = true
)
```

```
Inference results:
  Posteriors       | available for (μ, τ)
  Free Energy:     | Real[14763.3, 3275.23, 711.581, 677.154, 677.154, 677.
154, 677.154, 677.154, 677.154, 677.154]
```



```julia
println(&quot;μ: mean = &quot;, mean(result.posteriors[:μ]), &quot;, std = &quot;, std(result.posteriors[:μ]))
```

```
μ: mean = -2.971872353197491, std = 0.014944631146411699
```



```julia
println(&quot;τ: mean = &quot;, mean(result.posteriors[:τ]), &quot;, std = &quot;, std(result.posteriors[:τ]))
```

```
τ: mean = 4.477428186915035, std = 0.20003673898580324
```




#### Manual inference


For advanced use cases it is advised to write inference functions manually as it provides more flexibility, here is an example of manual inference specification:

```julia
model, (μ, τ, y) = create_model(test_model6(length(dataset)), constraints = constraints6);
```



For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose `ReactiveMP` inference engine export the `setmarginal!` function:

```julia
setmarginal!(μ, vague(NormalMeanPrecision))
setmarginal!(τ, vague(GammaShapeRate))
```


```julia
μ_values = keep(Marginal)
τ_values = keep(Marginal)

μ_subscription = subscribe!(getmarginal(μ), μ_values)
τ_subscription = subscribe!(getmarginal(τ), τ_values)

for i in 1:10
    update!(y, dataset)
end
```


```julia
getvalues(μ_values)
```

```
10-element Vector{Marginal}:
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-2.9778227486057664e-9, w
=0.010000001002000566))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.305541720402683, w=9.
197972258055358))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-8961.35878923723, w=3015
.3948179642957))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13299.939039330546, w=44
75.2726473581515))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13306.368334888894, w=44
77.436024661919))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13306.374754430191, w=44
77.438184757114))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13306.374760836907, w=44
77.4381869128165))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13306.374760843295, w=44
77.43818691497))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13306.374760843308, w=44
77.438186914974))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-13306.374760843311, w=44
77.438186914974))
```



```julia
getvalues(τ_values)
```

```
10-element Vector{Marginal}:
 Marginal(GammaShapeRate{Float64}(a=501.0, b=5.000000000045277e14))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=54527.80939349945))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=166.14794802151863))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.94873675084978))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.89464599536808))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.89459201279816))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.89459195892357))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.89459195886992))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.89459195886981))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=111.89459195886978))
```



```julia
println(&quot;μ: mean = &quot;, mean(last(μ_values)), &quot;, std = &quot;, std(last(μ_values)))
```

```
μ: mean = -2.971872353197491, std = 0.014944631146411699
```



```julia
println(&quot;τ: mean = &quot;, mean(last(τ_values)), &quot;, std = &quot;, std(last(τ_values)))
```

```
τ: mean = 4.477428186915035, std = 0.20003673898580324
```




### Form constraints

In order to support form constraints, the `randomvar()` function also supports a `where { ... }` clause with some optional arguments. One of these arguments is `form_constraint` that allows us to specify a form constraint to the random variables in our model. Another one is `prod_constraint` that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.

&lt;img style=&quot;display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;&quot; src=&quot;./pics/posterior.png&quot; /&gt;

```julia
@model function test_model7_with_manual_constraints(n)
    τ ~ Gamma(shape = 1.0, rate = 1.0) 
    
    # In case of form constraints `randomvar()` call is necessary
    μ = randomvar() where { marginal_form_constraint = PointMassFormConstraint() }
    μ ~ Normal(mean = 0.0, variance = 100.0)
    
    y = datavar(Float64, n)
    
    for i in 1:n
        y[i] ~ Normal(mean = μ, precision = τ) where { q = q(y[i])q(μ)q(τ) }
    end
    
    return μ, τ, y
end
```



As in the previous example we can use `@constraints` macro to achieve the same goal with a nicer syntax:

```julia
constraints7 = @constraints begin 
    q(μ) :: PointMass
    
    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`
end
```

```
Constraints:
  marginals form:
    q(μ) :: PointMassFormConstraint() [ prod_constraint = ProdGeneric(fallb
ack = ProdAnalytical()) ]
  messages form:
  factorisation:
    q(μ, τ) = q(μ)q(τ)
Options:
  warn = true
```




In this example we specified an extra constraints for $q_i$ for Bethe factorisation:

$$
q(s) = \prod_{a \in \mathcal{V}} q_a(s_a) \prod_{i \in \mathcal{E}} q_i^{-1}(s_i)
$$

```julia
@model function test_model7(n)
    τ ~ Gamma(shape = 1.0, rate = 1.0) 
    
    # In case of form constraints `randomvar()` call is necessary
    μ = randomvar()
    μ ~ Normal(mean = 0.0, variance = 100.0)
    
    y = datavar(Float64, n)
    
    for i in 1:n
        y[i] ~ Normal(mean = μ, precision = τ)
    end
    
    return μ, τ, y
end
```


```julia
model, (μ, τ, y) = create_model(test_model7(length(dataset)), constraints = constraints7);
```


```julia
setmarginal!(μ, vague(NormalMeanPrecision))
setmarginal!(τ, PointMass(1.0))

μ_values = keep(Marginal)
τ_values = keep(Marginal)

μ_subscription = subscribe!(getmarginal(μ), μ_values)
τ_subscription = subscribe!(getmarginal(τ), τ_values)

for i in 1:10
    update!(y, dataset)
end
```


```julia
getvalues(μ_values) |&gt; last
```

```
Marginal(PointMass{Float64}(-2.971872359821657))
```



```julia
getvalues(τ_values) |&gt; last
```

```
Marginal(GammaShapeRate{Float64}(a=501.0, b=111.78292095877487))
```




By default `RxInfer` tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two messages in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.


```julia
μ = randomvar() where { 
    prod_constraint = ProdGeneric(),
    form_constraint = SampleListFormConstraint() 
}
```


Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. The `ReactiveMP` inference engine exports a special `prod_constraint` called `ProdPreserveType` especially for that purpose:

```julia
μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }
```


**Note**: `@constraints` macro specifies required `prod_constraint` automatically.


### Free Energy


During variational inference the `RxInfer` optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the `score` function.

```julia
model, (μ, τ, y) = create_model(test_model6(length(dataset)), constraints = constraints6);
```


```julia
bfe_observable = score(model, Float64, BetheFreeEnergy())
```

```
ProxyObservable(Float64, MapProxy(Tuple{ReactiveMP.CountingReal{Float64}, R
eactiveMP.CountingReal{Float64}}))
```



```julia
bfe_subscription = subscribe!(bfe_observable, (fe) -&gt; println(&quot;Current BFE value: &quot;, fe));
```


```julia
# Reset the model with vague marginals
setmarginal!(μ, vague(NormalMeanPrecision))
setmarginal!(τ, vague(GammaShapeRate))

for i in 1:10
    update!(y, dataset)
end
```

```
Current BFE value: 689.7201594542112
Current BFE value: 677.1544242374439
Current BFE value: 677.1544239885261
Current BFE value: 677.1544239885247
Current BFE value: 677.1544239885247
Current BFE value: 677.1544239885252
Current BFE value: 677.1544239885252
Current BFE value: 677.1544239885252
Current BFE value: 677.1544239885252
Current BFE value: 677.1544239885252
```



```julia
# It always necessary to unsubscribe and release computer resources
unsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])
```



### Meta data specification

During model specification some functional dependencies may accept an optional `meta` object in the `where { ... }` clause. The purpose of the `meta` object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The `meta` object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:

```julia
# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of 
# the autoregressive process and its order. In addition it specifies that the message computation rules should
# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations
# by cost of possible numerical instabilities during an inference procedure
s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }
...
s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }
```

Another example with `GaussianControlledVariance`, or simply `GCV` [see Hierarchical Gaussian Filter], node:

```julia
# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` 
# method with `21` sigma points for approximation of non-lineariety between hierarchy layers
xt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }
```


The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.


### RxInfer.jl `@meta` macro


Users can use `@meta` macro from the `RxInfer.jl` package to achieve the same goal. Read more about `@meta` macro in the corresponding documentation section. Here is a simple example of the same meta specification:

```julia
@meta begin 
     AR(s, θ, γ) -&gt; ARMeta(Multivariate, 5, ARsafe())
end
```

```
Meta specification:
  AR(s, θ, γ) -&gt; ARMeta{Multivariate, ARsafe}(5, ARsafe())
Options:
  warn = true
```




## Creating custom nodes and message computation rules


### Custom nodes

To create a custom functional form and to make it available during model specification the `ReactiveMP` inference engine exports the `@node` macro:

```julia
# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:
@node NormalMeanVariance Stochastic [ out, μ, v ]

# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification
@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]

# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error
struct NormalMeanVariance end 

@node NormalMeanVariance Stochastic [ out, μ, v ]

# It is also possible to use function objects as a node functional form
function dot end

# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them 
# out = dot(x, a)
@node typeof(dot) Deterministic [ out, x, a ]
```


**Note**: Deterministic nodes do not support factorisation constraints with the `where { q = ... }` clause.

After that it is possible to use the newly created node during model specification:

```julia
@model function test_model()
    ...
    y ~ dot(x, a)
    ...
end
```


### Custom messages computation rules

`RxInfer.jl` exports the `@rule` macro to create custom message computation rules. For example let us create a simple `+` node to be available for usage in the model specification usage. We refer to *A Factor Graph Approach to Signal Modelling , System Identification and Filtering* [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the `+` node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for `+` node is the following:

$$\begin{aligned}
\mu_z = \mu_x + \mu_y\\
V_z = V_x + V_y
\end{aligned}\]</p><p>To specify this in <code>RxInfer.jl</code> we use the <code>@node</code> and <code>@rule</code> macros:</p><pre><code class="language-julia hljs">@node typeof(+) Deterministic  [ z, x, y ]

@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin
    x_mean, x_var = mean_var(m_x)
    y_mean, y_var = mean_var(m_y)
    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)
end</code></pre><p>In this example, for the <code>@rule</code> macro, we specify a type of our functional form: <code>typeof(+)</code>. Next, we specify an edge we are going to compute an outbound message for. <code>Marginalisation</code> indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:</p><p class="math-container">\[\begin{aligned}
q(z) = \int q(z, x, y) \mathrm{d}x\mathrm{d}y
\end{aligned}\]</p><p>If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:</p><p>&lt;div style=&quot;width:100%&quot;&gt; &lt;div style=&quot;width:50%; left:25%; position: relative; padding-top: 50px; padding-bottom: 50px&quot;&gt; &lt;img style=&quot;display: block;   margin-left: auto;   margin-right: auto;   width: 50%;&quot; src=&quot;./pics/sp.png&quot; align=&quot;left&quot;/&gt;</p><p>&lt;img style=&quot;display: block;   margin-left: auto;   margin-right: auto;   width: 50%;&quot; src=&quot;./pics/vmp.png&quot; align=&quot;left&quot; /&gt; &lt;/div&gt; &lt;div style=&quot;width:50%&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/div&gt; &lt;/div</p><p>&lt;div style=&quot;width:100%&quot;&gt; <span>$\begin{aligned} \mu(z) = \int f(x, y, z)\mu(x)\mu(y)\mathrm{d}x\mathrm{d}y \end{aligned}$</span></p><p class="math-container">\[\begin{aligned}
\nu(z) = \exp{ \int \log f(x, y, z)q(x)q(y)\mathrm{d}x\mathrm{d}y }
\end{aligned}\]</p><p>&lt;/div&gt;</p><p>The <code>@rule</code> macro supports both cases with special prefixes during rule specification:</p><ul><li><code>m_</code> prefix corresponds to the incoming message on a specific edge</li><li><code>q_</code> prefix corresponds to the posterior marginal of a specific edge</li></ul><p>Example of a Sum-Product rule with <code>m_</code> messages used:</p><pre><code class="language-julia hljs">@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin 
    m_out_mean, m_out_cov = mean_cov(m_out)
    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))
end</code></pre><p>Example of a Variational rule with Mean-Field assumption with <code>q_</code> posteriors used:</p><pre><code class="language-julia hljs">@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin 
    return NormalMeanPrecision(mean(q_out), mean(q_τ))
end</code></pre><p><code>RxInfer.jl</code> also supports structured rules. It is possible to obtain joint marginal over a set of edges:</p><pre><code class="language-julia hljs">@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin
    m, V = mean_cov(q_out_μ)
    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))
    α = convert(typeof(θ), 1.5)
    return Gamma(α, θ)
end</code></pre><p><strong>NOTE</strong>: In the <code>@rule</code> specification the messages or marginals arguments <strong>must</strong> be in order with interfaces specification from <code>@node</code> macro:</p><pre><code class="language-julia hljs"># Inference backend expects arguments in `@rule` macro to be in the same order
@node NormalMeanPrecision Stochastic [ out, μ, τ ]</code></pre><p>Any rule always has access to the meta information with hidden the <code>meta::Any</code> variable:</p><pre><code class="language-julia hljs">@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin 
    ...
    println(meta)
    ...
end</code></pre><p>It is also possible to dispatch on a specific type of a meta object:</p><pre><code class="language-julia hljs">@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin 
    ...
end</code></pre><p>or</p><pre><code class="language-julia hljs">@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin 
    ...
end</code></pre><h3 id="Customizing-messages-computational-pipeline"><a class="docs-heading-anchor" href="#Customizing-messages-computational-pipeline">Customizing messages computational pipeline</a><a id="Customizing-messages-computational-pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#Customizing-messages-computational-pipeline" title="Permalink"></a></h3><p>In certain situations it might be convenient to customize the default message computational pipeline. <code>RxInfer.jl</code> supports the <code>pipeline</code> keyword in the <code>where { ... }</code> clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.</p><p>&lt;img style=&quot;display: block;   margin-left: auto;   margin-right: auto;   width: 30%;&quot; src=&quot;./pics/pipeline.png&quot; width=&quot;20%&quot; /&gt;</p><pre><code class="language-julia hljs"># Logs all outbound messages
y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }
# Initialise messages to be vague
y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }
# In principle, it is possible to approximate outbound messages with Laplace Approximation
y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }</code></pre><p>Let us return to the coin toss model, but this time we want to print flowing messages:</p><pre><code class="language-julia hljs">@model function coin_toss_model_log(n)

    y = datavar(Float64, n)

    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(&quot;θ&quot;) }

    for i in 1:n
        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(&quot;y[$i]&quot;) }
    end
    
    return y, θ
end</code></pre><pre><code class="language-julia hljs">_, (y, θ) = RxInfer.create_model(coin_toss_model_log(5));</code></pre><pre><code class="language-julia hljs">θ_subscription = subscribe!(getmarginal(θ), (value) -&gt; println(&quot;New posterior marginal for θ: &quot;, value));</code></pre><pre><code class="nohighlight hljs">[θ][Beta][out]: VariationalMessage()</code></pre><pre><code class="language-julia hljs">coinflips = float.(rand(Bernoulli(0.5), 5));</code></pre><pre><code class="language-julia hljs">update!(y, coinflips)</code></pre><pre><code class="nohighlight hljs">[y[1]][Bernoulli][p]: VariationalMessage()
[y[2]][Bernoulli][p]: VariationalMessage()
[y[3]][Bernoulli][p]: VariationalMessage()
[y[4]][Bernoulli][p]: VariationalMessage()
[y[5]][Bernoulli][p]: VariationalMessage()
New posterior marginal for θ: Marginal(Beta{Float64}(α=4.0, β=10.0))</code></pre><pre><code class="language-julia hljs">unsubscribe!(θ_subscription)</code></pre><pre><code class="language-julia hljs"># Inference is lazy and does not send messages if no one is listening for them
update!(y, coinflips)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Overview/">« Overview</a><a class="docs-footer-nextpage" href="../Assessing People Skills/">Assessing People Skills »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 3 November 2022 14:21">Thursday 3 November 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
